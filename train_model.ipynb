{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh5JS4CNnQDZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
        "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk, pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for file paths\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# tf.debugging.set_log_device_placement(enabled=True)\n",
        "# setup GPU\n",
        "device_name = tf.test.gpu_device_name() if tf.test.gpu_device_name() else '/cpu:0'\n",
        "device_name\n",
        "# tf.device(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "okrG10o-orrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Getter Class\n",
        "class SentenceGetter(object):\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "metadata": {
        "id": "VqOCFHxrp2zO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data = pd.read_csv(userdata.get('dataset'), encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "_ugotTupnsFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process\n",
        "\n",
        "total_w = len(list(data['Word'].values))\n",
        "words = list(set(data[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "num_words = len(words)\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "num_tags = len(tags)\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n",
        "\n",
        "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=num_words - 1)\n",
        "\n",
        "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
      ],
      "metadata": {
        "id": "k9X0jVinqBeM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# For GPU, creating constants\n",
        "if tf.device(device_name) == '/device:GPU:0':\n",
        "  x_train = tf.constant(x_train)\n",
        "  x_test = tf.constant(x_test)\n",
        "  y_train = tf.constant(y_train)\n",
        "  y_test = tf.constant(y_test)"
      ],
      "metadata": {
        "id": "MvQBnQNjrANj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "\n",
        "input_word = Input(shape=(max_len,))\n",
        "\n",
        "model = Embedding(input_dim=num_words, output_dim=50, input_length=max_len)(input_word)\n",
        "model = SpatialDropout1D(0.1)(model)\n",
        "model = Bidirectional(LSTM(units=100, return_sequences=True))(model)\n",
        "out = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(model)\n",
        "model = Model(inputs=input_word, outputs=out, name=\"NER_Model\")\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ip8SnKwRsFNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Model\n",
        "\n",
        "chkpt = ModelCheckpoint(\"model_weights.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                        save_weights_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=0,\n",
        "                                mode='max', baseline=None, restore_best_weights=False)\n",
        "\n",
        "callbacks = [chkpt, early_stopping]\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 100\n",
        "\n",
        "# reduce batch_size and epochs for cpu\n",
        "if tf.device(device_name) != '/device:GPU:0':\n",
        "  batch_size = 64\n",
        "  epochs = 10\n",
        "\n",
        "model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), shuffle=True,\n",
        "          batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=1)\n"
      ],
      "metadata": {
        "id": "0q9vBwA2VSZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "kQtUqlRoVxDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "if tf.device(device_name) != '/device:GPU:0':\n",
        "  model.save(userdata.get('model_cpu'))\n",
        "else:\n",
        "  model.save(userdata.get('model_gpu'))"
      ],
      "metadata": {
        "id": "EG8Icrctssqj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "\n",
        "model = None\n",
        "\n",
        "if tf.device(device_name) != '/device:GPU:0':\n",
        "  model = models.load_model(userdata.get('model_cpu'))\n",
        "else:\n",
        "  model = models.load_model(userdata.get('model_gpu'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2_3b0KkZiTSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save data\n",
        "np.save(userdata.get('data'), (word2idx, max_len, tags))"
      ],
      "metadata": {
        "id": "ziqJ9x7u-I4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "word2idx, max_len, tags = np.load(userdata.get('data'), allow_pickle=True)"
      ],
      "metadata": {
        "id": "FtmKXfMW_BgQ"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentence drom test data set\n",
        "import re\n",
        "\n",
        "index = 9000\n",
        "\n",
        "tokens = []\n",
        "for x in x_test[index]:\n",
        "    if not words[x] == 'ENDPAD':\n",
        "        tokens.append(words[x - 1] + ' ')\n",
        "for j, word in enumerate(tokens):\n",
        "    if re.findall(r'^[&().,?]', word) or (re.findall(r\"^[']\", word) and tokens[j - 1][-1] == ' '):\n",
        "        tokens[j - 1] = tokens[j - 1][:-1]\n",
        "sentence = ''.join(tokens)\n",
        "sentence"
      ],
      "metadata": {
        "id": "hajzPJ9GzZPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess input sentence\n",
        "\n",
        "# sentence = \"The International Committee of the Red Cross helped organize the initiative, which is the first of its kind. \"\n",
        "\n",
        "sentence = nltk.word_tokenize(sentence)\n",
        "\n",
        "x_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in sentence]],\n",
        "                            padding=\"post\", value=0, maxlen=max_len)\n",
        "\n",
        "\n",
        "# Predicting\n",
        "p = model.predict(np.array([x_sent[0]]))\n",
        "p = np.argmax(p, axis=-1)\n",
        "\n",
        "data = [[w, tags[p]] for w, p in zip(sentence, p[0])]\n",
        "\n",
        "for (w, t) in data:\n",
        "  print(\"{:15}: {:5}\".format(w, t))"
      ],
      "metadata": {
        "id": "gwSOPzsLrBwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post process predictions\n",
        "\n",
        "# grouping words e.g. B-gpe and next immediate I-gpe will be combined as one B-gpe\n",
        "def group_words(l):\n",
        "    group = [l[0]]\n",
        "    for item in l[1:]:\n",
        "        if 'I-' in item[1]:\n",
        "            group[-1][0] += ' ' + item[0]\n",
        "        else:\n",
        "            group.append(item)\n",
        "    return group\n",
        "\n",
        "processed_data = []\n",
        "for word, tag in group_words(data):\n",
        "  word = word + ' '\n",
        "  if tag == 'O':\n",
        "    processed_data.append(word)\n",
        "  elif tag == 'B-nat':\n",
        "    processed_data.append((word, 'Nat'))\n",
        "  elif tag == 'B-org':\n",
        "    processed_data.append((word, 'Organization'))\n",
        "  elif tag == 'B-art':\n",
        "    processed_data.append((word, 'Art'))\n",
        "  elif tag == 'B-tim':\n",
        "    processed_data.append((word, 'Time'))\n",
        "  elif tag == 'B-geo':\n",
        "    processed_data.append((word, 'Location'))\n",
        "  elif tag == 'B-eve':\n",
        "    processed_data.append((word, 'Event'))\n",
        "  elif tag == 'B-gpe':\n",
        "    processed_data.append((word, 'Geo-Political'))\n",
        "  elif tag == 'B-per':\n",
        "    processed_data.append((word, 'Person'))\n",
        "\n",
        "# display final results\n",
        "processed_data"
      ],
      "metadata": {
        "id": "l-YEyK1rtrL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}